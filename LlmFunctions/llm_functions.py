from langchain import PromptTemplate, LLMChain
from langchain.chat_models import ChatOpenAI
import json


llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)


def fix_json(json_to_fix):
    template = """I have some broken JSON below that I need to be able to run json.loads() on.  Can you fix it for me? Thanks.\n\n{question}"""
    prompt_from_template = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt_from_template,llm=llm, verbose=True)
    response = llm_chain.run(json_to_fix)
    return response

def genMemoriesFromBackstory(backstory: str)->list:
    '''the purpose of this function is to generate a list of observations/memories
    from the written prose version of the backstory of an NPC (or maybe another entity, i dont know).
    These can then be utilized during game-init to generate an initial starting index of NPC
    LTM (long term memories).  This function is to be generated by the game editor.  The memories are 
    passed back to the game editor, then saved in MongoDB.  They're loaded during game-init and turned into 
    a game-specific, user-specific vector index.'''
    template = """I want you to convert an NPC agent backstory into a set of observations where an observation is an event directly perceived by that agent. Common observations include behaviors performed by the agent themselves or behaviors that agents perceive being performed by other agents or non-agent objects.  Utilize proper nouns wherever possible.
    
    '''''
    The backstory is: {backstory}
    
    '''''
    You must format your output as a JSON dictionary that adheres to the following JSON schema instance:
    'observations': The list of observations
    """
    prompt_from_template = PromptTemplate(template=template, input_variables=["backstory"])
    llm_chain = LLMChain(prompt=prompt_from_template,llm=llm, verbose=True)
    response = llm_chain.run(backstory)
    try:
        response = json.loads(response)
    except:
        response = fix_json(response)
        response = json.loads(response)
    return response['observations']

def genLTMQuestionsGivenMissionInfo(mission_brief: str, formatted_mission_outcomes: str)->list:
    '''Given a mission brief, we need to be able to extract how an NPC might react to/on that
    mission given their long term memories'''

    template = """Given the following mission brief and potential mission outcomes we need to generate a list of questions
    that can be used to query an NPC's long-term memory to pull out core NPC characteristics as well as to gather 
    relevant information and experiences that might impact the outcome or narrative of the mission:
        
    mission_brief: {mission_brief}

    potential mission outcomes: {formatted_mission_outcomes}
    
    '''''
    You must format your output as a JSON dictionary that adheres to the following JSON schema instance:
    'questions': The list of questions
    """
    prompt_from_template = PromptTemplate(template=template, input_variables=["mission_brief", "formatted_mission_outcomes"])
    llm_chain = LLMChain(prompt=prompt_from_template,llm=llm, verbose=True)
    response = llm_chain.run(mission_brief=mission_brief, formatted_mission_outcomes=formatted_mission_outcomes)
    try:
        response = json.loads(response)
    except:
        response = fix_json(response)
        response = json.loads(response)
    return response['questions']


def genKGQuestionsGivenMissionBrief(mission_brief: str)->list:
    '''Given a mission brief, we need to be able to extract pertinent knowledge from a knowledge graph. 
    This requires us to put together a list of questions to query the KG / vectorDB'''

    template = """given this mission brief: {mission_brief}

    '''''
    Objective: Extract and formulate relevant questions based on the mission details that can be used to query an NPC's long term memories and knowledge graph to gather pertinent information for the mission's success.  Keep questions general so as to best extract knowledge.

    '''''
    Process:
    Read and understand the mission brief.
    Identify key mission details, objectives, and potential challenges.
    Formulate questions that highlight knowledge or memory gaps based on mission details.
    
    '''''
    You must format your output as a JSON dictionary that adheres to the following JSON schema instance:
    'questions': The list of questions
    """
    prompt_from_template = PromptTemplate(template=template, input_variables=["mission_brief"])
    llm_chain = LLMChain(prompt=prompt_from_template,llm=llm, verbose=True)
    response = llm_chain.run(mission_brief)
    try:
        response = json.loads(response)
    except:
        response = fix_json(response)
        response = json.loads(response)
    return response['questions']




    